{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report - Project 1 - Navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```Navigation.ipynb``` notebook implements a deep neural network that converts the states of the agent to the corresponding actions that maximizes the expected total future reward on the Unity Banana environment.\n",
    "\n",
    "The next item are briefy descriptions of the steps that are performed on the ```Navigation.ipynb``` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Start the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of ```Navigation.ipynb``` notebook start the Unity environment where the simulations are performed. The ```Banana``` environment is used for this example. Other libraries are also loaded on this step such as ```pytorch``` and ```matplotlib```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the data structures of the states and action can be visualized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Agent Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step provides the agent training. Two auxiliary files ares used:\n",
    "- ```model.py```: implements the class QNetwork where a neural network is implemented. A two hidden layer with 128 neurons each is implemented. The first hidden layer is composed by a linear function with a relu activation. The second hidden layer is composed of linear functions.\n",
    "- ```dqn_agent.py```: implements the class Agent with the ```step```, ```act```, ```learn``` and ```soft_update``` methods. The ```step``` method evokes the ```learn``` function when necessary or just save the experiences in a buffer. The ```learn``` function updates the neural network weights based in order to minimize the loss function. The ```act``` function chooses the action based on the neural network or based on a epsilon-greedy policy. The ```soft_update```function updates the neural network weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Smart Agent performanceÂ¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step provides a visualization of the trained agent on three episodes and the associated scores."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
